{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 8: A construction of the Poisson process\n",
    "\n",
    "\n",
    "In this notebook we study a discrete-time process that behaves like the Poisson process at larger scales. One can also see this notebook as a <b> construction </b> of the Poisson process.\n",
    "\n",
    "Fix $T$ and $\\mu$ positive parameters and a very large integer $n$. We will see how we can naturally construct a Poisson process from a long random string of 0's and 1's.\n",
    "We take a long string of size $T\\times n$ of i.i.d. integers $(B_k)_k$ in $\\{0,1\\}$, with $\\mathbb{P}(B_k = 1) = 1-\\mathbb{P}(B_k = 0) = \\frac{\\mu}{n}$. We then record the instants at which the $1$'s appear (note that this has small proba) and check that the counting process of the $1$'s when time-rescaled by $N$ looks like a Poisson process up to time $T$.\n",
    "\n",
    "The string should look a bit like this:\n",
    "\\begin{equation}\n",
    "0000000000000000000000000 \\color{blue}{1} 00000000000000000000000000\\color{blue}{1}00000000000000000000\n",
    "\\end{equation}\n",
    "\n",
    "Denoting $S$ the time of appearance of the first $1$, we can see that\n",
    "\\begin{align}\n",
    "    \\mathbb{P}(S/n > t) &= \\mathbb{P}(S > nt) \\\\\n",
    "    &= \\mathbb{P}(B_k = 0 \\text{ for all }k\\leq nt) \\\\\n",
    "    &= (1-\\frac{\\mu}{n})^{nt} \\\\\n",
    "    &\\to e^{-\\mu t} \\text{ as } n \\text{ goes to infinity}.\n",
    "\\end{align}\n",
    "\n",
    "This suggests that when rescaling the time by $n$, the time of appearance of $1$ should have law Exp$(\\mu)$. And by independance of the bits in the string we see that any two consecutive $1$'s will be separated by an exponential random variable with parameter $\\mu$. <b>Thus the time-rescaled counting process of the ones should converge to a Poisson process.</b>\n",
    "\n",
    "This is what we shall see in the first part of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#length of the string\n",
    "n = 10**5 #it should go to infinity ideally but 10**5 is enough\n",
    "T = 50 \n",
    "mu = 1/5 #mu will ultimately be the rate of the Poisson process hidden in our construction \n",
    "\n",
    "\n",
    "# n should be seen as a quantity that will go to infinity and T as a fixed time horizon.\n",
    "# The string has total length n*T so that, once rescaled by n, the counting process will have length ~ T.\n",
    "S = np.random.choice(2, n*T, p=[1-mu/n,mu/n])# random integers from 0 (inclusive) to 2 (exclusive), size n.\n",
    "\n",
    "# There are on average n*T*mu/n = T*mu bits equal to 1. \n",
    "# Let's check that.\n",
    "print(\"T*mu = \", T*mu)\n",
    "print(\"Number of bits equal to 1 in the string S: \", np.sum(S))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_jumps = np.argwhere(S==1) # time_jumps is the vector containing the indices of S where there are ones. \n",
    "#it corresponds to the jumps of the rescaled process\n",
    "\n",
    "time_jumps = np.concatenate( time_jumps, axis=0 ) # we use this trick to have an array of reals instead of an array of array...\n",
    "\n",
    "scaled_time_jumps = time_jumps/n # we rescaled the time by n\n",
    "\n",
    "n_jumps = len(time_jumps)\n",
    "jumps = [i for i in range(n_jumps)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the plot\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 4))    \n",
    "ax.set_xlabel(\"rescaled time up to time T\")\n",
    "ax.set_ylabel(\"counting of 1's\")\n",
    "\n",
    "# Plot the theoretical pdf\n",
    "plt.step(scaled_time_jumps, jumps, label='counting process rescaled by n')\n",
    "#ax.legend(loc='best', frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will refer to the <b> time-rescaled </b>graph depicted above as the counting process $(C_t)_{0\\leq t \\leq T}$.\n",
    "At that point, we see that the counting process looks very much like a Poisson process so let us check that we indeed retrieve some properties of a Poisson process of rate $\\mu$ on it.\n",
    "\n",
    "\n",
    "### 1) The law of $C_T$\n",
    "For example, we first compute the empirical law of the counting process at time $T$ and compare it to the law of a Poisson process at time $T$. Recall that if $N$ is a poisson process of rate $\\mu$ then\n",
    "\\begin{equation}\n",
    "    \\mathbb{P}(N(T) = k) = e^{-\\mu T}\\frac{(\\mu T )^k}{k!}.\n",
    "\\end{equation}\n",
    "To plot the empirical distribution of $C_T$ we will simulate many times the above construction and plot the results in an histogram that we will compare to the theoretical distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "import math\n",
    "\n",
    "# input data\n",
    "N_sim = 10**3 # number of simulations\n",
    "n = 10**4 \n",
    "T = 50 \n",
    "mu = 1/5 \n",
    "\n",
    "C_T_array = np.zeros(N_sim)\n",
    "\n",
    "for i in range(N_sim):\n",
    "    S = np.random.choice(2, n*T, p=[1-mu/n,mu/n]) #create a new random string every time\n",
    "    C_T_array[i] = np.sum(S)\n",
    "    #C_T corresponds simply to the total number of ones in the string \n",
    "    #since this latter quantity is the number of jumps \n",
    "    #in the process C_t and thus its value at the final time t=T.\n",
    "\n",
    "# Plot the empirical histogram and the theoretical pdf for T to compare\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 4))    \n",
    "ax.set_xlabel(\"histogram bin\")\n",
    "ax.set_ylabel(\"distribution of C_T and N_T\")\n",
    "\n",
    "# Compute and plot the histogram\n",
    "weights = np.ones_like(C_T_array) / len(C_T_array) #this vector is used to normalize this histogram \n",
    "# so that the bars sum up to 1\n",
    "ti, bins, pr = ax.hist(C_T_array, 100, weights = weights, label = 'empirical distribution')\n",
    "\n",
    "# Specify the theoretical pdf in a good range of values\n",
    "a,b = math.ceil(bins[0]),math.ceil(bins[-1])\n",
    "f_T = np.zeros(b-a)\n",
    "for k in range(a,b):\n",
    "    f_T[k-a] = np.exp(-mu*T)*(mu*T)**k/math.factorial(k) #f is the (discrete) law of N_T\n",
    "    \n",
    "# Plot the theoretical pdf\n",
    "ax.plot(range(a,b),f_T,'--', label='Theoretical pdf')\n",
    "ax.legend(loc='best', frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the graphs look alike even though there is some apparent variability in the results. Since $N$ and $n$ should both be very large to have interesting results you can check that decreasing one of these parameters leads to less accuracy in the prediction.\n",
    "\n",
    "### 2) The distribution of the instant of the first jump in $(C_t)_{0\\leq t \\leq T}$\n",
    "Another thing that we can look at is the time of the first jump, we should recover that it is approximately given by an exponential distribution with parameter $\\mu$. The larger $n$ and $N$ are, the better the approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "import math\n",
    "\n",
    "# input data\n",
    "N_sim = 10**3 # number of simulations\n",
    "n = 10**4 \n",
    "T = 70 \n",
    "mu = 1/5 \n",
    "\n",
    "C_T_array = np.zeros(N_sim)\n",
    "\n",
    "for i in range(N_sim):\n",
    "    S = np.random.choice(2, n*T, p=[1-mu/n,mu/n]) #create a new random string every time\n",
    "    time_jumps = np.argwhere(S==1)\n",
    "    C_T_array[i] = time_jumps[0]/n #Take the normalized time of the first jump \n",
    "    \n",
    "    #It is possible though with very small probability that S contains no 1. In this case, the cell\n",
    "    #will return an exception since time_jumps[0] will try to access the first element of an empty array.\n",
    "    #If this is the case, you can just rerun the cell which should be fine since this has very small probability.\n",
    "    #Otherwise you could modify the code to take into account these events into the description of the empirical distribution\n",
    "    #of the first jump.\n",
    "\n",
    "    \n",
    "# Plot the empirical histogram and the theoretical pdf for T to compare\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 4))    \n",
    "ax.set_xlabel(\"histogram bin\")\n",
    "ax.set_ylabel(\"distribution of the time of the first jump in C_t\")\n",
    "\n",
    "# Definite the theoretical pdf\n",
    "def pdf(x):\n",
    "    return mu*np.exp(-mu*x)\n",
    "# Compute and plot the histogram\n",
    "\n",
    "ti, bins, pr = ax.hist(C_T_array, 100, density = True, label = 'empirical distribution',alpha = 0.8)\n",
    "   \n",
    "# Plot the theoretical pdf\n",
    "x_axis = np.arange(0, bins[-1], 0.2)\n",
    "ax.plot(x_axis, [pdf(x) for x in x_axis], '--', label='Exp(mu) pdf')\n",
    "ax.legend(loc='best', frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) The distribution of the instant of later jumps in $(C_t)_{0\\leq t \\leq T}$\n",
    "\n",
    "By modifying slightly the code above, can you plot the empirical law of the instant of the $10$-th jump? What should it be theoretically and is it what you observe? \n",
    "\n",
    "Note that it is possible that you don't oberserve $10$ $1$'s in the string $S$ and therefore that the $10$-th jump does not exist. You may take $T$ large enough to have an overwhelming probability to see $10$ jumps or you can try to observe anterior jumps like the $4$ or $5$ jumps for example and then one does not need $T$ to be too large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "import math\n",
    "\n",
    "# input data\n",
    "N_sim = 10**3 # number of simulations\n",
    "n = 10**4 \n",
    "T = 200 \n",
    "mu = 1/5 \n",
    "n_jump = 10 #observe the 10-th jump\n",
    "\n",
    "C_T_array = np.zeros(N_sim)\n",
    "\n",
    "for i in range(N_sim):\n",
    "    S = np.random.choice(2, n*T, p=[1-mu/n,mu/n]) #create a new random string every time\n",
    "    time_jumps = np.argwhere(S==1)\n",
    "    C_T_array[i] = time_jumps[n_jump-1]/n #Take the normalized time of the n_jump-th jump\n",
    "    \n",
    "\n",
    "    \n",
    "# Plot the empirical histogram and the theoretical pdf \n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 4))    \n",
    "ax.set_xlabel(\"histogram bin\")\n",
    "ax.set_ylabel(\"distribution of the time of the first jump in C_t\")\n",
    "\n",
    "# Definite the theoretical pdf\n",
    "# The k-th jumps has law Gamma(k,mu)\n",
    "def pdf(x):\n",
    "    return mu*np.exp(-mu*x)*((mu*x)**(n_jump-1))/math.factorial(n_jump-1)\n",
    "\n",
    "# Compute and plot the histogram\n",
    "ti, bins, pr = ax.hist(C_T_array, 100, density = True, label = 'empirical distribution',alpha = 0.8)\n",
    "   \n",
    "# Plot the theoretical pdf\n",
    "x_axis = np.arange(0, bins[-1], 0.2)\n",
    "ax.plot(x_axis, [pdf(x) for x in x_axis], '--', label='Exp(mu) pdf')\n",
    "ax.legend(loc='best', frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 8 - Problem 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 - Empirical Distribution of the Poisson Process\n",
    "\n",
    "Use the code from last week's notebook to compare the empirical distribution of a Poisson process $N(t)$ and intensity (rate) $\\lambda$ (for example, use $t=100$, $\\lambda=0.2$) with the theoretical distribution found in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "Lambda = #ADD PARAMETER HERE\n",
    "T = # ADD PARAMETER HERE\n",
    "\n",
    "\n",
    "def sample_times(Lambda, N):\n",
    "    return np.random.exponential(1/Lambda, N)\n",
    "\n",
    "def num_copies_graded(tmax, Lambda):\n",
    "    t_array = np.linspace(0, tmax, num=100)\n",
    "    count=np.zeros(100)\n",
    "    event=0\n",
    "    while event < t_array[-1]:\n",
    "        Ti = sample_times(Lambda, 1) #We use the exponential sampling defined in previous block\n",
    "        event += Ti\n",
    "        count[t_array >= event] +=1\n",
    "    return count\n",
    "\n",
    "\n",
    "# Sample N(T) a number of times\n",
    "m = 10000\n",
    "Nt_array = np.zeros(m)\n",
    "for i in range(m):\n",
    "    N=num_copies_graded(T, Lambda)\n",
    "    Nt_array[i] = N[-1]\n",
    "\n",
    "# Plot the empirical histogram\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 4))    \n",
    "ax.set_xlabel(\"histogram bin\")\n",
    "ax.set_ylabel(\"Probability\")\n",
    "\n",
    "binwidth = 1\n",
    "bins = np.arange(min(Nt_array), max(Nt_array) + binwidth, binwidth)\n",
    "nt, bins, pr = ax.hist(Nt_array, bins=bins, density=True, alpha=0.6)\n",
    "\n",
    "# Specify the theoretical pdf in a good range of values\n",
    "a,b = math.ceil(bins[0]),math.ceil(bins[-1])\n",
    "f_T = np.zeros(b-a)\n",
    "for k in range(a,b):\n",
    "    f_T[k-a] = #ENTER THE THEORETICAL PDF FOR N(T) HERE\n",
    "\n",
    "# Plot the theoretical pdf\n",
    "ax.plot(range(a,b), f_T, '--', label='Theoretical pdf')\n",
    "ax.legend(loc='best', frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 - Non-homogeneous Poisson Process\n",
    "\n",
    "In this problem we will simulate non-homogeneous Poisson processes using this week's Jupyter notebook. Recall that for a non-negative function $\\lambda: \\mathbb{R}^+ \\to \\mathbb{R}^+$, the non-homogeneous Poisson process with intensity function $\\lambda$ is the unique counting process satisfying \n",
    "\n",
    "- $\\{N(t): t \\geq 0\\}$ has independent increments\n",
    "- $\\mathbb{P}(N(t+dt) - N(t) \\geq 2) = o(dt)$\n",
    "- $\\mathbb{P}(N(t+dt) - N(t) = 1) = \\lambda(t)dt + o(dt)$\n",
    "\n",
    "See Ross, section 5.4, for more details. Use the Jupyter notebook to simulate the non-homogeneous Poisson process with the following functions:\n",
    "\n",
    "**(a)** $\\lambda(t) = ct$ for a constant $c > 0$. Generate a histogram to find the distribution of $N(t)$ for a fixed $t$ -- what do you expect the distribution to be?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# input data\n",
    "T = #ADD PARAMETER HERE\n",
    "c = #ADD PARAMETER HERE\n",
    "N_sim = 10**3 # number of simulations\n",
    "\n",
    "Nt_array = np.zeros(N_sim)\n",
    "\n",
    "for i in range(N_sim):\n",
    "    # Simulate arrival times\n",
    "    arrival_times = [0]\n",
    "    while arrival_times[-1] < 0.5*c*T**2:\n",
    "        Ti = np.random.exponential(1)\n",
    "        new_arrival_time = arrival_times[-1] + Ti # Arrival time for Poisson(1) process\n",
    "        arrival_times.append(new_arrival_time)\n",
    "\n",
    "    # Rescale arrival times according to the intensity\n",
    "    rescaled_arrival_times = [np.sqrt((2/c)*t) for t in arrival_times]\n",
    "    \n",
    "    # Sample N(T) by counting the arrivals\n",
    "    count = 0\n",
    "    for t in rescaled_arrival_times:\n",
    "        if t < T:\n",
    "            count += 1\n",
    "            \n",
    "    Nt_array[i] = count\n",
    "    \n",
    "    \n",
    "# Plot the empirical histogram\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 4))    \n",
    "ax.set_xlabel(\"histogram bin\")\n",
    "ax.set_ylabel(\"Probability\")\n",
    "\n",
    "binwidth = 1\n",
    "bins = np.arange(min(Nt_array), max(Nt_array) + binwidth, binwidth)\n",
    "nt, bins, pr = ax.hist(Nt_array, bins=bins, density=True, alpha=0.6)\n",
    "\n",
    "\n",
    "# Specify the theoretical pdf in a good range of values\n",
    "a,b = math.ceil(bins[0]),math.ceil(bins[-1])\n",
    "f_T = np.zeros(b-a)\n",
    "for k in range(a,b):\n",
    "    f_T[k-a] = #ENTER THE THEORETICAL PDF FOR N(T) HERE\n",
    "\n",
    "# Plot the theoretical pdf\n",
    "ax.plot(range(a,b), f_T, '--', label='Theoretical pdf')\n",
    "ax.legend(loc='best', frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** $\\lambda(t) = e^{-ct}$ for a constant $c > 0$. Argue that $N(t)$ is bounded as $t \\to \\infty$. Generate a histogram for $\\lim_{t\\to \\infty} N(t)$ by sampling at a large time $t$ -- what do you expect the distribution to be?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# input data\n",
    "T = #ADD PARAMETER HERE (Recommeded to not exceed 10**4 or running time will be very long)\n",
    "c = #ADD PARAMETER HERE (Recommended value: 0.01, note that choosing c too small or large will cause numerical issues)\n",
    "N_sim = 10**3 # number of simulations\n",
    "\n",
    "lambda_t = lambda t : np.exp(-c*t)\n",
    "Nt_array = np.zeros(N_sim)\n",
    "\n",
    "\n",
    "# In part (b) we simulate N(t) using a thinning procedure\n",
    "def thinning_probability(t, lambda_t_max): \n",
    "    return lambda_t(t)/lambda_t_max\n",
    "    \n",
    "\n",
    "def simulate_Nt(T):\n",
    "    t=0\n",
    "    count=0\n",
    "    lambda_t_max = 1 # since the lambda_t is a decreasing function the max occurs at t=0\n",
    "    \n",
    "    while t<= T:\n",
    "        r = np.random.uniform(0,1)\n",
    "        t = t - math.log(r)/lambda_t_max\n",
    "        s = np.random.uniform(0,1)\n",
    "        \n",
    "        if s <= thinning_probability(t, lambda_t_max):\n",
    "            count += 1\n",
    "            \n",
    "    return count\n",
    "    \n",
    "    \n",
    "# Simulate N(t) a number of times to generate the date for the histrogram\n",
    "for i in range(N_sim):      \n",
    "    Nt_array[i] = simulate_Nt(T)\n",
    "    \n",
    "    \n",
    "# Plot the empirical histogram\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 4))    \n",
    "ax.set_xlabel(\"histogram bin\")\n",
    "ax.set_ylabel(\"Probability\")\n",
    "\n",
    "binwidth = 1\n",
    "bins = np.arange(min(Nt_array), max(Nt_array) + binwidth, binwidth)\n",
    "nt, bins, pr = ax.hist(Nt_array, bins=bins, density=True, alpha=0.6)\n",
    "\n",
    "\n",
    "# Specify the theoretical pdf in a good range of values\n",
    "a,b = math.ceil(bins[0]),math.ceil(bins[-1])\n",
    "f_T = np.zeros(b-a)\n",
    "for k in range(a,b):\n",
    "    f_T[k-a] = #ENTER THE THEORETICAL PDF FOR N(T) HERE\n",
    "\n",
    "# Plot the theoretical pdf\n",
    "ax.plot(range(a,b), f_T, '--', label='Theoretical pdf')\n",
    "ax.legend(loc='best', frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**(c)** $\\lambda(t) = c \\cdot 1\\{\\lfloor t \\rfloor \\text{ is even}\\}$ for a constant $c > 0$. Generate a histogram for $N(t)$: is it Poisson? How is this process related to the usual Poisson process with rate $c$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# input data\n",
    "T = #ADD PARAMETER HERE\n",
    "c = #ADD PARAMETER HERE\n",
    "N_sim = 10**3 # number of simulations\n",
    "\n",
    "lambda_t = lambda t : c*(1 - (math.floor(t) % 2))\n",
    "Nt_array = np.zeros(N_sim)\n",
    "\n",
    "\n",
    "# We simulate N(t) for part (c) using a thinning procedure as in part (b)\n",
    "def thinning_probability(t, lambda_t_max): \n",
    "    return lambda_t(t)/lambda_t_max\n",
    "    \n",
    "\n",
    "def simulate_Nt(T):\n",
    "    t=0\n",
    "    count=0\n",
    "    lambda_t_max = c # since lambda_t takes value 0 or value c > 0 for all t\n",
    "    \n",
    "    while t<= T:\n",
    "        r = np.random.uniform(0,1)\n",
    "        t = t - math.log(r)/lambda_t_max\n",
    "        s = np.random.uniform(0,1)\n",
    "        \n",
    "        if s <= thinning_probability(t, lambda_t_max):\n",
    "            count += 1\n",
    "            \n",
    "    return count\n",
    "    \n",
    "    \n",
    "# Simulate N(t) a number of times to generate the data for the histogram\n",
    "for i in range(N_sim):      \n",
    "    Nt_array[i] = simulate_Nt(T)\n",
    "    \n",
    "    \n",
    "# Plot the empirical histogram\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 4))    \n",
    "ax.set_xlabel(\"histogram bin\")\n",
    "ax.set_ylabel(\"Probability\")\n",
    "\n",
    "binwidth = 1\n",
    "bins = np.arange(min(Nt_array), max(Nt_array) + binwidth, binwidth)\n",
    "nt, bins, pr = ax.hist(Nt_array, bins=bins, density=True, alpha=0.6)\n",
    "\n",
    "\n",
    "# Specify the theoretical pdf in a good range of values\n",
    "a,b = math.ceil(bins[0]),math.ceil(bins[-1])\n",
    "f_T = np.zeros(b-a)\n",
    "\n",
    "for k in range(a,b):\n",
    "    if math.floor(T) % 2 == 0: # T is even\n",
    "        #ENTER THE THEORETICAL PDF FOR N(T) HERE FOR WHEN T IS EVEN\n",
    "    else: # T is odd\n",
    "        #ENTER THE THEORETICAL PDF FOR N(T) HERE FOR WHEN T IS ODD\n",
    "        \n",
    "# Plot the theoretical pdf\n",
    "ax.plot(range(a,b), f_T, '--', label='Theoretical pdf')\n",
    "ax.legend(loc='best', frameon=False)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
